# üó∫Ô∏è Laboratorio Integrador - An√°lisis Geoespacial de Comuna Chilena

[![GitHub](https://img.shields.io/badge/GitHub-franciscoparrao-blue?style=flat&logo=github)](https://github.com/franciscoparrao)
[![Course](https://img.shields.io/badge/Curso-Geoinform√°tica-green)](https://github.com/franciscoparrao/geoinformatica)
[![License](https://img.shields.io/badge/License-MIT-yellow)](LICENSE)

## üìã Descripci√≥n

Proyecto integrador que combina todas las tecnolog√≠as y m√©todos aprendidos en las primeras 7 clases del curso de Geoinform√°tica. Este laboratorio requiere desarrollar un an√°lisis territorial completo de una comuna chilena, incluyendo procesamiento de datos espaciales, geoestad√≠stica, machine learning y visualizaci√≥n interactiva.

## üë• Informaci√≥n del Equipo

| Integrante | Rol | GitHub |
|------------|-----|--------|
| [Nombre 1] | [Rol/Responsabilidad] | [@usuario1] |
| [Nombre 2] | [Rol/Responsabilidad] | [@usuario2] |

**Comuna seleccionada:** [NOMBRE DE LA COMUNA]
**Repositorio del curso:** [github.com/franciscoparrao/geoinformatica](https://github.com/franciscoparrao/geoinformatica)

## üöÄ Quick Start

### Prerrequisitos

- Docker Desktop instalado (versi√≥n 4.0+)
- Python 3.10 o superior
- Git
- M√≠nimo 8GB RAM disponible
- 20GB de espacio en disco

### Instalaci√≥n R√°pida

```bash
# 1. Clonar el repositorio
git clone https://github.com/franciscoparrao/geoinformatica.git
cd geoinformatica/laboratorio-integrador

# 2. Ejecutar script de configuraci√≥n
chmod +x setup.sh
./setup.sh

# 3. Configurar variables de entorno
cp .env.example .env
# Editar .env con tus valores

# 4. Levantar servicios Docker
docker-compose up -d

# 5. Verificar instalaci√≥n
docker-compose ps
```

### Acceso a Servicios

| Servicio | URL | Credenciales |
|----------|-----|--------------|
| Jupyter Lab | http://localhost:8888 | Token en .env |
| PostGIS | localhost:5432 | geouser/geopass |
| Web App | http://localhost:5000 | - |
| PgAdmin | http://localhost:5050 | admin@geoinformatica.cl/admin |

## üìÅ Estructura del Proyecto

```
laboratorio_integrador/
‚îú‚îÄ‚îÄ üìÑ README.md                 # Este archivo
‚îú‚îÄ‚îÄ üìã requirements.txt          # Dependencias Python
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml        # Configuraci√≥n Docker
‚îú‚îÄ‚îÄ üîí .env                      # Variables de entorno (no subir!)
‚îú‚îÄ‚îÄ üìù .gitignore               # Archivos ignorados
‚îÇ
‚îú‚îÄ‚îÄ üê≥ docker/                  # Configuraciones Docker
‚îÇ   ‚îú‚îÄ‚îÄ jupyter/                # Imagen personalizada Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ postgis/                # Inicializaci√≥n PostGIS
‚îÇ   ‚îî‚îÄ‚îÄ web/                    # Aplicaci√≥n web
‚îÇ
‚îú‚îÄ‚îÄ üíæ data/                    # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ raw/                    # Datos originales sin procesar
‚îÇ   ‚îú‚îÄ‚îÄ processed/              # Datos procesados y limpios
‚îÇ   ‚îî‚îÄ‚îÄ external/               # Datos de fuentes externas
‚îÇ
‚îú‚îÄ‚îÄ üìì notebooks/               # An√°lisis en Jupyter
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_acquisition.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_exploratory_analysis.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_geostatistics.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_machine_learning.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 05_results_synthesis.ipynb
‚îÇ
‚îú‚îÄ‚îÄ üêç scripts/                 # Scripts Python reutilizables
‚îÇ   ‚îú‚îÄ‚îÄ download_data.py       # Descarga automatizada
‚îÇ   ‚îú‚îÄ‚îÄ process_data.py        # Procesamiento
‚îÇ   ‚îú‚îÄ‚îÄ spatial_analysis.py    # An√°lisis espacial
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Funciones auxiliares
‚îÇ
‚îú‚îÄ‚îÄ üåê app/                    # Aplicaci√≥n web Streamlit
‚îÇ   ‚îú‚îÄ‚îÄ main.py               # Aplicaci√≥n principal
‚îÇ   ‚îú‚îÄ‚îÄ pages/                # P√°ginas del dashboard
‚îÇ   ‚îî‚îÄ‚îÄ components/           # Componentes reutilizables
‚îÇ
‚îú‚îÄ‚îÄ üìä outputs/                # Resultados generados
‚îÇ   ‚îú‚îÄ‚îÄ figures/              # Gr√°ficos y mapas
‚îÇ   ‚îú‚îÄ‚îÄ models/               # Modelos ML entrenados
‚îÇ   ‚îî‚îÄ‚îÄ reports/              # Informes y documentos
‚îÇ
‚îî‚îÄ‚îÄ üìö docs/                   # Documentaci√≥n
    ‚îú‚îÄ‚îÄ guia_usuario.md       # Manual de usuario
    ‚îú‚îÄ‚îÄ arquitectura.md       # Arquitectura t√©cnica
    ‚îî‚îÄ‚îÄ api_reference.md      # Referencia API
```

## üõ†Ô∏è Configuraci√≥n Detallada

### 1. Configuraci√≥n del Entorno Python

```bash
# Crear ambiente virtual
python -m venv venv
```

Activaci√≥n seg√∫n tu shell / SO:

PowerShell (Windows):
```powershell
# Si ves error de ExecutionPolicy, primero:
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned

# Activar
./venv/Scripts/Activate.ps1
```

CMD (Windows cl√°sico):
```cmd
venv\Scripts\activate.bat
```

Git Bash (Windows):
```bash
source venv/Scripts/activate  # Nota: En Windows la carpeta es Scripts, no bin
```

Linux / macOS:
```bash
source venv/bin/activate
```

Verificar versi√≥n de Python dentro del entorno:
```powershell
python --version
```

Instalar dependencias del proyecto (host):
```powershell
pip install -r requirements.txt
```

Si el comando `python` falla en PowerShell, prueba:
```powershell
py -3 -m venv venv
py -3 --version
```

Desactivar el entorno:
```powershell
deactivate
```

Problemas comunes:
- ExecutionPolicy bloquea Activate.ps1: aplicar `Set-ExecutionPolicy` como arriba.
- Usaste `source venv/bin/activate` en Windows: reemplazar por ruta `Scripts`.
- Carpeta `venv` corrupta: borrar `venv/` y recrear.
- M√∫ltiples instalaciones de Python: confirmar con `where python` (PowerShell) o `Get-Command python`.
```

### 2. Configuraci√≥n de PostGIS

```bash
# Conectarse a la base de datos (servicio 'postgis')
docker compose exec postgis psql -U geouser -d geodatabase

# Verificar extensiones
\dx

# Debe mostrar:
# - postgis
# - postgis_topology
# - postgis_raster
# - pgrouting (si est√° instalado)
```

### 3. Descarga de Datos

```powershell
# Ejecutar script (desde la ra√≠z del proyecto)
python scripts/download_data.py --comuna "La Florida" --sources all

# Omitir WFS IDE Chile si est√° lento:
python scripts/download_data.py --comuna "La Florida" --sources all --skip-wfs

# Solo OSM:
python scripts/download_data.py --comuna "La Florida" --sources osm

# Cambiar directorio de salida:
python scripts/download_data.py --comuna "La Florida" --output data/raw --sources all

# Opciones:
# --comuna     (obligatorio)
# --output     Directorio destino (default: data/raw)
# --sources    osm | ide | all (default: all)
# --skip-wfs   Omite descarga de l√≠mites administrativos (WFS)
```

> Nota: El par√°metro `--year` mostrado anteriormente no est√° implementado en `scripts/download_data.py`. Si se requiere una noci√≥n de a√±o, se puede a√±adir luego como nueva opci√≥n CLI.

### 3.1 DEM (SRTM) y Sistema de Referencia (CRS)

La descarga del Modelo Digital de Elevaci√≥n (DEM) es parte de la ETAPA DE ADQUISICI√ìN y se activa incluyendo `srtm` en `--sources`.
Los productos derivados (pendiente, orientaci√≥n, NDVI, estad√≠sticas zonales) pertenecen a la ETAPA DE PROCESAMIENTO y se agregar√°n m√°s adelante (no los genera el script de descarga actual).

Archivos generados actualmente cuando se incluye la fuente `srtm`:
- `srtm_dem.tif` (EPSG:4326) DEM recortado a la comuna.
- `srtm_dem_32719.tif` (EPSG:32719) DEM reproyectado a UTM Zona 19 Sur para an√°lisis m√©trico.

Fallback: Si todos los mirrors SRTM fallan, el c√≥digo intenta Copernicus DEM GLO-30 (v√≠a STAC) usando un bbox expandido. En ese caso se generan:
- `copernicus_dem.tif` (EPSG:4326)
- `copernicus_dem_32719.tif` (EPSG:32719)

Orden de mirrors SRTM intentados por tile:
1. `https://srtm.kurviger.de/SRTM3/{code}.hgt.gz`
2. `https://dds.cr.usgs.gov/srtm/version2_1/SRTM3/South_America/{code}.hgt.zip`
3. `https://srtm.kurviger.de/SRTM3/{code}.hgt`
4. `https://srtmtiles.s3.amazonaws.com/{code}.hgt.gz`
5. `https://s3.amazonaws.com/elevation-tiles-prod/skadi/{prefix}/{code}.hgt.gz` (Mapzen/Skadi)

L√≥gica principal:
- Se calcula la bounding box de la comuna y se determinan los c√≥digos de tile por la esquina Suroeste (ej. `S33W071`).
- Se descargan s√≥lo los tiles m√≠nimos requeridos (comunas peque√±as suelen usar un √∫nico tile).
- Caso √∫nico tile: se recorta directamente; caso m√∫ltiple: mosaico con `rasterio.merge` y luego recorte.
- Se guarda el DEM en EPSG:4326 y luego se reproyecta autom√°ticamente a EPSG:32719.

¬øPor qu√© EPSG:32719?
- Zona UTM 19 Sur cubre gran parte de Chile central (distancias y √°reas en metros).
- Evita distorsiones de c√°lculos m√©tricos en lat/long (EPSG:4326).
- Consistencia con an√°lisis posteriores (slope/aspect, buffers, interpolaciones, joins espaciales en PostGIS).

Verificaci√≥n r√°pida:
```python
import rasterio, numpy as np
for f in ["data/raw/srtm_dem.tif", "data/raw/srtm_dem_32719.tif"]:
    with rasterio.open(f) as src:
        arr = src.read(1, masked=True)
        print(f, "CRS=", src.crs, "shape=", src.shape, "min/max=", np.nanmin(arr), np.nanmax(arr))
```

Pr√≥ximos pasos (procesamiento):
- Generar `slope.tif` y `aspect.tif` desde `srtm_dem_32719.tif`.
- Calcular `sentinel2_ndvi.tif` (+ reproyecci√≥n `sentinel2_ndvi_32719.tif`).
- Estad√≠sticas zonales (elevaci√≥n media, NDVI medio) por manzana censal.
- Ingesta de DEM y derivados a PostGIS.

Estas tareas se documentar√°n en una secci√≥n futura de "Procesamiento de Rasters".

### 4. Carga y Procesamiento Inicial (OSM + DEM + Enriquecimientos)

La ingesta y primeros derivados se ejecutan con `scripts/process_data.py`. Se implement√≥ autodetecci√≥n de rutas para edificios, amenidades y el tile SRTM (`*.hgt`).

#### 4.1 Comando m√≠nimo (ingesta OSM autodetectada)
```powershell
python scripts/process_data.py --load-osm --srid 32719 --index
```

#### 4.2 Especificar rutas manuales (PowerShell)
Usar backtick `` ` `` para continuar l√≠nea (no usar `^` en PowerShell):
```powershell
python scripts/process_data.py --load-osm `
    --buildings data/raw/osm_buildings.geojson `
    --amenities data/raw/osm_amenities.geojson `
    --schema raw_data `
    --srid 32719 `
    --index
```
Una sola l√≠nea equivalente:
```powershell
python scripts/process_data.py --load-osm --buildings data/raw/osm_buildings.geojson --amenities data/raw/osm_amenities.geojson --schema raw_data --srid 32719 --index
```

#### 4.3 Dentro del contenedor Jupyter
```bash
docker exec -it jupyter_lab python /home/jovyan/scripts/process_data.py --load-osm \
    --schema raw_data \
    --srid 32719 --index
```
Si los archivos est√°n en `/home/jovyan/data/raw/` no hace falta indicar rutas.

#### 4.4 Derivados del DEM (slope/aspect)
```powershell
python scripts/process_data.py --dem-derivatives
```
Combinar ingesta OSM y derivados:
```powershell
python scripts/process_data.py --load-osm --srid 32719 --index --dem-derivatives
```
Genera:
- `data/raw/srtm_dem.tif`
- `data/raw/srtm_dem_32719.tif`
- `data/processed/slope.tif`
- `data/processed/aspect.tif`

#### 4.5 Flags futuros (pendientes)
Los siguientes flags ya est√°n a√±adidos en la CLI. Algunos requieren que existan previamente los archivos de entrada en `data/raw/`.

| Flag | Entrada esperada | Salida en `data/processed/` | Descripci√≥n |
|------|------------------|------------------------------|-------------|
| `--ndvi` | `sentinel_B04.tif`, `sentinel_B08.tif` | `sentinel2_ndvi.tif`, `sentinel2_ndvi_<SRID>.tif` | Calcula NDVI y reproyecta si se usa `--srid` |
| `--join-censo` | Carpeta `Censo2017_ManzanaEntidad_CSV/` y `manzanas_censales.geojson` | `manzanas_atributos.geojson` | Une microdatos INE con geometr√≠as de manzana (heur√≠stica de clave) |
| `--join-uso-suelo` | `uso_suelo_minvu.geojson`, `manzanas_censales.geojson` | `manzanas_uso_suelo.geojson` | Intersecta para obtener √°rea zonificada y conteo de zonas |
| `--metrics` | Manzanas, edificios, amenidades + (opcional joins previos) | `metrics_manzanas.csv` | Conteos de edificios, amenidades, √°rea y agrega atributos disponibles |

Ejemplos de ejecuci√≥n:
```powershell
# NDVI (si existen bandas Sentinel-2)
python scripts/process_data.py --ndvi --srid 32719

# Join Censo y Uso Suelo en una sola corrida
python scripts/process_data.py --join-censo --join-uso-suelo

# M√©tricas luego de haber generado los joins previos
python scripts/process_data.py --metrics

# Pipeline combinado OSM + DEM + Censo + Uso Suelo + M√©tricas
python scripts/process_data.py --load-osm --dem-derivatives --join-censo --join-uso-suelo --metrics --srid 32719 --index
```

Consideraciones:
- Si un archivo de salida ya existe se reutiliza para mantener idempotencia.
- `--join-censo` intenta detectar autom√°ticamente la columna clave (ej. `MANZENT`). Si no la encuentra, selecciona la primera columna com√∫n; revisar log si falta alguna variable.
- `--join-uso-suelo` agrega columnas `area_zonas` y `zonas_count` al resultado por manzana.
- `--metrics` agrega columnas de conteo y √°rea; si existen outputs de censo y uso suelo los incorpora mediante join.

#### 4.6 Extensiones recientes (NDVI flexible, uso suelo unificado, red vial, cat√°logo raster)

Se han incorporado nuevas capacidades al script `scripts/process_data.py` para enriquecer el pipeline:

| Flag | Entradas Clave | Salidas | Detalles |
|------|----------------|---------|----------|
| `--unify-uso-suelo` | Shapefiles en `data/raw/uso_suelo_minvu/**` (PRC*, PRMS*, LU*) | `uso_suelo_unificado.geojson` | Unifica m√∫ltiples fuentes, normaliza campo `categoria` y agrega `source`|
| `--network-metrics` | `osm_network.graphml`, `manzanas_censales.geojson` | `network_nodes_metrics.geojson`, `network_metrics.csv` | Centralidades (degree, betweenness) y densidad vial por manzana |
| `--ndvi` | Bandas B04/B08 (nombres soportados) | `sentinel2_ndvi.tif`, `sentinel2_ndvi_<SRID>.tif` | Detecci√≥n flexible nombres: `sentinel_B04/B08`, `sentinel2_B04/B08`, fallback `*_B04/_B08` |
| `--ingest-processed` | Archivos en `data/processed` | Tablas en esquema `processed_data` + `raster_catalog` | Carga `.geojson` y `.csv`; cataloga metadatos `.tif` |

Detecci√≥n flexible NDVI:
1. Busca pares exactos (`sentinel_B04.tif`, `sentinel_B08.tif`).
2. Luego (`sentinel2_B04.tif`, `sentinel2_B08.tif`).
3. Fallback: primer par que cumpla patr√≥n gen√©rico `*_B04.tif` y `*_B08.tif`.
Si dimensiones o transform difieren se aborta para evitar NDVI inv√°lido.

Cat√°logo raster (`processed_data.raster_catalog`):
- Se crea autom√°ticamente al ejecutar `--ingest-processed` si existen `.tif` en `data/processed`.
- Guarda metadatos (crs, ancho/alto, bounds, transform, nodata, dtype, band_count) para reproducibilidad.
- No almacena los p√≠xeles en la base (optimiza espacio); el filesystem sigue siendo fuente de verdad.

Consulta r√°pida en contenedor PostGIS:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT filename, crs, width, height, band_count FROM processed_data.raster_catalog;"
```

En Windows PowerShell local (si tienes `psql` instalado):
```powershell
psql -h localhost -p 5432 -U geouser -d geodatabase -c "SELECT * FROM processed_data.raster_catalog LIMIT 5;"
```
Nota: No ejecutes `SELECT ...` directamente en PowerShell sin `psql -c`; PowerShell interpretar√° `SELECT` como comando de su propio pipeline.

#### 4.7 Tabla de trazabilidad Raw ‚Üí Processed

| Archivo Raw | Tipo | Transformaci√≥n / Acci√≥n | Archivo(s) Processed | Flag / Paso |
|-------------|------|-------------------------|----------------------|-------------|
| `osm_buildings.geojson` | Vector | Reproyecci√≥n opcional, ingesta PostGIS | (tabla `raw_data.osm_buildings`) | `--load-osm` |
| `osm_amenities.geojson` | Vector | Reproyecci√≥n opcional, ingesta PostGIS | (tabla `raw_data.osm_amenities`) | `--load-osm` |
| `osm_network.graphml` | Grafo | Parse nodos/aristas, reproyecci√≥n | `network_nodes_metrics.geojson`, `network_metrics.csv` | `--network-metrics` |
| `manzanas_censales.geojson` | Vector | Base para joins y m√©tricas | `manzanas_atributos.geojson`, `manzanas_uso_suelo.geojson`, `metrics_manzanas.csv` | `--join-censo`, `--join-uso-suelo`, `--metrics` |
| Censo CSV (`Censo2017_Manzanas.csv`) | Tabular | Limpieza separador y normalizaci√≥n clave | `manzanas_atributos.geojson` | `--join-censo` |
| Uso suelo (PRC*, PRMS*, LU*) | Vector | Unificaci√≥n + normalizaci√≥n categor√≠as | `uso_suelo_unificado.geojson` | `--unify-uso-suelo` |
| `uso_suelo_minvu.geojson` | Vector | Intersecci√≥n con manzanas | `manzanas_uso_suelo.geojson` | `--join-uso-suelo` |
| `S34W071.hgt` | Raster | Conversi√≥n HGT‚ÜíGeoTIFF, recorte comuna, reproyecci√≥n | `srtm_dem.tif`, `srtm_dem_32719.tif`, `slope.tif`, `aspect.tif` | `--dem-derivatives` |
| Bandas Sentinel (B04/B08) | Raster | C√°lculo NDVI + reproyecci√≥n | `sentinel2_ndvi.tif`, `sentinel2_ndvi_<SRID>.tif` | `--ndvi` |

Reglas de almacenamiento:
- `data/raw/`: s√≥lo fuentes originales sin modificaci√≥n (HGT, GraphML, shapefiles, GeoJSON base, bandas).
- `data/processed/`: productos derivados, agregados, reproyectados, enriquecidos o normalizados.
- Evitar copiar archivos sin transformaci√≥n a `processed/` para no duplicar almacenamiento; usar tabla de trazabilidad para referencia.
- Rasters derivados se catalogan en `processed_data.raster_catalog` al ejecutar `--ingest-processed`.

Ejemplo de pipeline completo (incluyendo catalogaci√≥n raster):
```powershell
python scripts/process_data.py \
    --load-osm --dem-derivatives --ndvi --join-censo --join-uso-suelo \
    --metrics --unify-uso-suelo --network-metrics --srid 32719
python scripts/process_data.py --ingest-processed --processed-schema processed_data
```

#### 4.8 Ingesta m√≠nima de fuentes base

La bandera `--ingest-minimum` carga en PostGIS las fuentes originales m√≠nimas (l√≠mite oficial, manzanas censales, uso_suelo_minvu consolidado, microdatos censo) y cataloga los rasters base (DEM y bandas Sentinel) sin mover p√≠xeles a la base.

Acciones:
- Vectoriales ‚Üí tablas (`comuna_boundaries_oficial`, `manzanas_censales`, `uso_suelo_minvu`).
- Microdatos censo ‚Üí tabla `censo_microdatos` (primeras columnas para ligereza).
- Rasters base ‚Üí filas en `{schema}.raster_catalog` con `source_group='raw'`.

Comando recomendado (a√±ade √≠ndices y reproyecci√≥n):
```powershell
python scripts/process_data.py --ingest-minimum --srid 32719 --index
```
Luego ejecutar derivados y productos y cargarlos al esquema de procesados:
```powershell
python scripts/process_data.py --dem-derivatives --ndvi --join-censo --join-uso-suelo --metrics --unify-uso-suelo --network-metrics --srid 32719
python scripts/process_data.py --ingest-processed --processed-schema processed_data
```

Verificaci√≥n r√°pida de m√©tricas:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) FROM processed_data.metrics_manzanas;"
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT AVG(ndvi_mean) FROM processed_data.metrics_manzanas;"
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT AVG(road_density_m_per_km2) FROM processed_data.network_metrics;"
```

#### 4.8.1 Validaci√≥n de claves hu√©rfanas (Censo ‚Üî Manzanas) y cobertura

Objetivo: comprobar qu√© microdatos del censo no encuentran geometr√≠a y la cobertura (%) de manzanas con atributos censales / m√©tricas derivadas.

1. Listar columnas disponibles en la tabla de microdatos (para confirmar nombre real de la clave):
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT column_name FROM information_schema.columns WHERE table_schema='raw_data' AND table_name='censo_microdatos';"
```

2. Contar total de registros y total de manzanas:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) AS total_microdatos FROM raw_data.censo_microdatos;"
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) AS total_manzanas FROM raw_data.manzanas_censales;"
```

3. Detecci√≥n de claves hu√©rfanas (usa COALESCE para variantes comunes de la clave):
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) AS microdatos_sin_geometria FROM raw_data.censo_microdatos c LEFT JOIN raw_data.manzanas_censales m ON COALESCE(c.manzent::text, c.id_manzent::text, c.mz_ent::text, c.manzana::text) = m.manzent::text WHERE m.manzent IS NULL;"
```

4. Proporci√≥n de microdatos que s√≠ tienen geometr√≠a:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT ROUND(100.0*COUNT(*)/(SELECT COUNT(*) FROM raw_data.censo_microdatos),2) AS pct_microdatos_con_geometria FROM raw_data.censo_microdatos c JOIN raw_data.manzanas_censales m ON COALESCE(c.manzent::text, c.id_manzent::text, c.mz_ent::text, c.manzana::text) = m.manzent::text;"
```

5. Cobertura de manzanas con atributos censales en producto procesado (`manzanas_atributos`):
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT ROUND(100.0*COUNT(*)/(SELECT COUNT(*) FROM processed_data.manzanas_atributos),2) AS pct_manzanas_con_microdatos FROM processed_data.manzanas_atributos WHERE manzent IS NOT NULL;"
```

6. Estad√≠sticos NDVI para validar rango esperado (solo valores no nulos):
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT MIN(ndvi_mean), MAX(ndvi_mean), AVG(ndvi_mean) FROM processed_data.metrics_manzanas WHERE ndvi_mean IS NOT NULL;"
```

7. (Opcional) Crear vista con clave can√≥nica para futuras consultas:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "CREATE OR REPLACE VIEW raw_data.censo_microdatos_canon AS SELECT COALESCE(manzent, id_manzent, mz_ent, manzana) AS manzent_canon, * FROM raw_data.censo_microdatos;"
```

Notas:
- No usar barras invertidas `\"` en PowerShell: simplemente encierra todo el SQL entre comillas dobles.
- Si la columna aparece como `ID_MANZENT` o `MZ_ENT`, el script de ingest m√≠nima la normaliza a `MANZENT` (nuevo ajuste). Si a√∫n no existe, usar la variante adecuada en COALESCE.
- Postgres convierte identificadores no entrecomillados a min√∫sculas; `MANZENT` y `manzent` son equivalentes si no se usan comillas dobles alrededor del nombre.
- El CSV original del censo trae a veces espacios finales en los headers (ej. `PUEBLO  `). El script ahora limpia (strip) todos los nombres y unifica la clave a `MANZENT` cuando encuentra alias (`ID_MANZENT`, `MZ_ENT`, `MANZANA`). Si ves ambas columnas `MZ_ENT` y `MANZENT`, la primera se dej√≥ porque ya exist√≠a la segunda antes de normalizar.

#### 4.8.2 NDVI y M√©tricas Zonales

El NDVI se genera con el flag `--ndvi` usando bandas B04 (red) y B08 (nir). Se soportan nombres:
`sentinel_B04.tif` / `sentinel_B08.tif`, `sentinel2_B04.tif` / `sentinel2_B08.tif`, o cualquier par `*_B04.tif` y `*_B08.tif` (fallback).

Comando de generaci√≥n (reproyecta si se entrega `--srid`):
```powershell
python scripts/process_data.py --ndvi --srid 32719
```
Salida:
- `data/processed/sentinel2_ndvi.tif`
- `data/processed/sentinel2_ndvi_32719.tif` (si se solicit√≥ reproyecci√≥n)

Registro en cat√°logo: al existir `raw_data.raster_catalog` se inserta metadato con `source_group='derived'`.

Para agregar la media de NDVI por manzana (estad√≠stica zonal) ejecutar luego:
```powershell
python scripts/process_data.py --metrics --srid 32719
```
Si el NDVI ya exist√≠a, la funci√≥n lo reutiliza. La columna resultante en `metrics_manzanas.csv` es `ndvi_mean`.

Validaci√≥n r√°pida de rango t√≠pico NDVI (-1 a 1):
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT MIN(ndvi_mean), MAX(ndvi_mean), AVG(ndvi_mean) FROM processed_data.metrics_manzanas WHERE ndvi_mean IS NOT NULL;"
```

#### 4.8.3 Registro y Manejo de Fila Hu√©rfana del Censo

Se detect√≥ una fila hu√©rfana (microdato sin geometr√≠a asociada) con clave:
```
MANZENT = 13129991999999
```
Patr√≥n sugiere c√≥digo especial (relleno / fuera de l√≠mite). Recomendaci√≥n: excluir esta fila en an√°lisis estad√≠sticos y mapas para evitar sesgos.

SQL para excluirla en consultas posteriores:
```sql
SELECT * FROM raw_data.censo_microdatos WHERE manzent <> '13129991999999';
```

Python (GeoPandas/Pandas) al cargar atributos:
```python
df = df[df['manzent'] != '13129991999999']
```

Si aparecieran m√°s casos, generar reporte actualizado:
```powershell
python scripts/report_orphans.py --output data/processed/orphans_microdatos.csv
```

#### 4.8.4 DEM Copernicus (Opcional)

Si se a√±ade archivo `copernicus_dem.tif` (o se descarga v√≠a flujo externo), se puede ejecutar reproyecci√≥n y c√°lculo de derivados para comparar calidad vs SRTM:
```powershell
python scripts/process_data.py --dem-derivatives --srid 32719
```
El script intentar√° crear `copernicus_dem_32719.tif` y reutilizar derivados si ya existen `slope.tif` / `aspect.tif`. Para mantener trazabilidad, documentar en el cat√°logo raster la procedencia (`source_group='raw'` para DEM base Copernicus, `source_group='derived'` para derivados).

Comparaci√≥n r√°pida de estad√≠sticas b√°sicas entre DEMs (en Python interactivo):
```python
import rasterio, numpy as np
for f in ["data/raw/srtm_dem_32719.tif", "data/raw/copernicus_dem_32719.tif"]:
    if not Path(f).exists():
        print("Falta", f); continue
    with rasterio.open(f) as src:
        arr = src.read(1, masked=True)
        print(f, "min", float(np.nanmin(arr)), "max", float(np.nanmax(arr)), "mean", float(np.nanmean(arr)))
```

Justificaci√≥n opcional en informe: Copernicus GLO-30 suele ofrecer menor ruido y mayor cobertura homog√©nea; comparar histograma y pendiente media puede apoyar elecci√≥n final de DEM para an√°lisis topogr√°fico avanzado.

### Buenas Pr√°cticas de Reproducibilidad
- Mantener `.env` con variables de conexi√≥n; no subir credenciales reales.
- Ejecutar scripts con flags expl√≠citos para poder reconstruir outputs.
- Versionar s√≥lo derivados compactos; evitar subir tiles raster gigantes si no cambian.
- Usar logs para verificar idempotencia ("ya existe, reutilizando").

---

Sigue la documentaci√≥n original abajo para fases anal√≠ticas avanzadas.

#### Verificaci√≥n de carga OSM
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) AS edificios FROM raw_data.osm_buildings;"
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT COUNT(*) AS amenidades FROM raw_data.osm_amenities;"
```
Confirmar SRID:
```powershell
docker compose exec postgis psql -U geouser -d geodatabase -c "SELECT 'osm_buildings' AS tabla, Find_SRID('raw_data','osm_buildings','geometry') UNION ALL SELECT 'osm_amenities', Find_SRID('raw_data','osm_amenities','geometry');"
```
Notas:
- Si omites `--srid` se conserva el CRS original.
- Esquema destino por defecto: `raw_data`.
- Autodetecci√≥n busca `data/raw/osm_buildings.geojson` y `data/raw/osm_amenities.geojson`.
- Para red vial `.graphml` la ingesta a√∫n es un placeholder.

### Acceso a Jupyter Lab

Al abrir `http://localhost:8888` se solicitar√° "Password or token". Usa el valor de `JUPYTER_TOKEN` definido en tu archivo `.env` (actual: `laboratorio2025`). Tambi√©n puedes obtener el token desde los logs del contenedor si se configurara din√°micamente.

## üìä Flujo de Trabajo

### Fase 1: Preparaci√≥n de Datos (Semana 1)

- [ ] Seleccionar comuna de estudio
- [ ] Configurar ambiente de desarrollo
- [ ] Descargar datos de m√∫ltiples fuentes
- [ ] Cargar datos en PostGIS
- [ ] Validar calidad de datos

### Fase 2: An√°lisis Espacial (Semana 2)

- [ ] An√°lisis exploratorio (ESDA)
- [ ] Calcular autocorrelaci√≥n espacial
- [ ] Identificar hot spots y clusters
- [ ] Crear visualizaciones tem√°ticas
- [ ] An√°lisis geoestad√≠stico

### Fase 3: Machine Learning y Aplicaci√≥n (Semana 3)

- [ ] Feature engineering espacial
- [ ] Entrenar modelos predictivos
- [ ] Validaci√≥n espacial
- [ ] Desarrollar aplicaci√≥n web
- [ ] Documentar resultados

## üî¨ An√°lisis Incluidos

### 1. An√°lisis Exploratorio de Datos Espaciales (ESDA)
- Estad√≠sticas descriptivas espaciales
- Mapas de distribuci√≥n
- An√°lisis de patrones

### 2. Autocorrelaci√≥n Espacial
- √çndice de Moran Global
- LISA (Local Indicators of Spatial Association)
- Getis-Ord G*

### 3. Geoestad√≠stica
- Semivariogramas
- Kriging ordinario
- Validaci√≥n cruzada

### 4. Machine Learning Geoespacial
- Random Forest espacial
- XGBoost con features geogr√°ficos
- Validaci√≥n espacial (no random!)

### 5. Visualizaci√≥n Interactiva
- Dashboard Streamlit
- Mapas interactivos con Folium
- Gr√°ficos din√°micos con Plotly

## üåê Aplicaci√≥n Web

### Ejecutar la aplicaci√≥n

```bash
# Desarrollo
streamlit run app/main.py

# Producci√≥n con Docker
docker-compose up web
```

### Caracter√≠sticas principales

- üó∫Ô∏è Mapa interactivo con m√∫ltiples capas
- üìà Gr√°ficos din√°micos de estad√≠sticas
- ü§ñ Panel de predicciones ML
- üíæ Descarga de resultados
- üì± Dise√±o responsive

## üìù Notebooks

### 1. `01_data_acquisition.ipynb`
Descarga y carga inicial de datos desde m√∫ltiples fuentes.

### 2. `02_exploratory_analysis.ipynb`
ESDA completo con visualizaciones y estad√≠sticas.

### 3. `03_geostatistics.ipynb`
An√°lisis geoestad√≠stico y interpolaci√≥n espacial.

### 4. `04_machine_learning.ipynb`
Modelos predictivos con validaci√≥n espacial.

### 5. `05_results_synthesis.ipynb`
S√≠ntesis de resultados y conclusiones.

## üß™ Testing

```bash
# Ejecutar tests unitarios
pytest tests/

# Ejecutar con coverage
pytest --cov=scripts tests/

# Verificar calidad del c√≥digo
flake8 scripts/ app/
black --check scripts/ app/
```

## üìà Monitoreo y Logs

```bash
# Ver logs de todos los servicios
docker-compose logs -f

# Logs de un servicio espec√≠fico
docker-compose logs -f postgis

# Estado de los contenedores
docker stats
```

## üêõ Troubleshooting

### Problema: Puerto en uso
```bash
# Verificar puertos en uso
sudo lsof -i :8888
sudo lsof -i :5432

# Matar proceso
kill -9 [PID]
```

### Problema: Falta de memoria Docker
```bash
# Aumentar memoria en Docker Desktop
# Settings -> Resources -> Memory -> 8GB m√≠nimo
```

### Problema: Error de permisos
```bash
# Linux/Mac
sudo chown -R $USER:$USER .

# Dar permisos de ejecuci√≥n
chmod +x scripts/*.py
```

## üìö Recursos y Referencias

### Documentaci√≥n Oficial
- [GeoPandas](https://geopandas.org)
- [PySAL](https://pysal.org)
- [OSMnx](https://osmnx.readthedocs.io)
- [Streamlit](https://docs.streamlit.io)
- [PostGIS](https://postgis.net/docs/)

### Fuentes de Datos
- [IDE Chile](https://www.ide.cl)
- [INE Chile](https://www.ine.cl)
- [OpenStreetMap](https://www.openstreetmap.org)
- [Google Earth Engine](https://earthengine.google.com)
- [Sentinel Hub](https://www.sentinel-hub.com)

### Tutoriales Recomendados
- [Automating GIS Processes](https://automating-gis-processes.github.io)
- [Geographic Data Science](https://geographicdata.science)
- [Spatial Thoughts](https://spatialthoughts.com)

## üë®‚Äçüíª Desarrollo

### Convenciones de c√≥digo

- Python: PEP 8
- Commits: Conventional Commits
- Branches: `feature/nombre`, `fix/nombre`, `docs/nombre`

### Flujo de Git

```bash
# Crear rama para nueva caracter√≠stica
git checkout -b feature/analisis-clustering

# Hacer cambios y commit
git add .
git commit -m "feat: agregar an√°lisis de clustering DBSCAN"

# Push y crear PR
git push origin feature/analisis-clustering
```

## üìä M√©tricas del Proyecto

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![PostGIS](https://img.shields.io/badge/PostGIS-3.3-green)
![License](https://img.shields.io/badge/License-MIT-yellow)

### Estad√≠sticas de c√≥digo

- L√≠neas de c√≥digo Python: [XXX]
- Notebooks Jupyter: 5
- Tests escritos: [XX]
- Coverage: [XX]%

## üìÑ Licencia

Este proyecto fue desarrollado como parte del curso de Geoinform√°tica en USACH.

## üôè Agradecimientos

- Prof. Francisco Parra O. por la gu√≠a y ense√±anza
- Compa√±eros de curso por el feedback
- Comunidad Open Source por las herramientas

## üìß Contacto

Para consultas sobre el proyecto:
- Email: [tu-email@usach.cl]
- GitHub Issues: [github.com/franciscoparrao/geoinformatica/issues](https://github.com/franciscoparrao/geoinformatica/issues)
- Repositorio: [github.com/franciscoparrao/geoinformatica](https://github.com/franciscoparrao/geoinformatica)

---

**√öltima actualizaci√≥n:** $(date)

**Estado del proyecto:** üöß En desarrollo

```mermaid
graph LR
    A[Datos Raw] --> B[Procesamiento]
    B --> C[PostGIS]
    C --> D[An√°lisis Espacial]
    D --> E[Machine Learning]
    E --> F[Web App]
    F --> G[Usuario Final]
```